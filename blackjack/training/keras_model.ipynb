{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3IFbPbiE25B",
        "outputId": "ef706a0a-a991-4ae0-9a08-45dc646d9e8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/annoskriver/.pyenv/versions/3.10.6/envs/blackjack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.config import set_visible_devices, get_visible_devices\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import keras_cv\n",
        "from keras_cv import bounding_box\n",
        "from keras_cv import visualization\n",
        "\n",
        "#comment this @Nikita\n",
        "from interface.params import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill this out @Nikita\n",
        "\n",
        "# TRAIN_DIR_IMGS  = '/Users/annoskriver/.data/blackjack/original_images/train/images'\n",
        "# TRAIN_DIR_LABELS = '/Users/annoskriver/.data/blackjack/original_images/train/labels'\n",
        "\n",
        "# VALID_DIR_IMGS = '/Users/annoskriver/.data/blackjack/original_images/valid/images'\n",
        "# VALID_DIR_LABELS = '/Users/annoskriver/.data/blackjack/original_images/valid/labels'\n",
        "\n",
        "# TEST_DIR_IMGS = '/Users/annoskriver/.data/blackjack/original_images/test/images'\n",
        "# TEST_DIR_LABELS = '/Users/annoskriver/.data/blackjack/original_images/test/labels'\n",
        "\n",
        "# MODEL_PATH = '/Users/annoskriver/.data/blackjack/models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M3yaL9RdwNkI"
      },
      "outputs": [],
      "source": [
        "SPLIT_RATIO = 0.2\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCH = 128\n",
        "GLOBAL_CLIPNORM = 10.0      \n",
        "IMAGE_SIZE = 416\n",
        "USE_GPU = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#delete GPU from visible devices, if not used\n",
        "if not USE_GPU:\n",
        "    set_visible_devices([], 'GPU')\n",
        "\n",
        "#look at visible devices\n",
        "get_visible_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q7CoR-VMwNkI"
      },
      "outputs": [],
      "source": [
        "# Class mapping\n",
        "class_ids = ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qWn-iRtAwNkI"
      },
      "outputs": [],
      "source": [
        "def get_labels(labels_dir: str, img_size: int = 416) -> (list, list):\n",
        "    file_paths = []\n",
        "    classes = []\n",
        "    boxes = []\n",
        "    # Get labels: class ID and x, y, width, height (bounding boxes)\n",
        "    for root, dirs, files in os.walk(labels_dir):\n",
        "        for file in files:\n",
        "            # Get path for each label .txt file\n",
        "            label_path = os.path.join(labels_dir, os.path.splitext(file)[0] + \".txt\")\n",
        "            file_paths.append(label_path)\n",
        "\n",
        "        # Sort paths by picture name\n",
        "        pattern = r\"(\\d+)_jpg\\.rf\"\n",
        "        file_paths.sort(key=lambda x: int(re.findall(pattern, x)[0]))\n",
        "\n",
        "        for file in file_paths:\n",
        "            # Open label .txt file\n",
        "            with open (file, \"r\") as label_file:\n",
        "                label_lines = label_file.readlines()\n",
        "\n",
        "                img_classes = []\n",
        "                img_boxes = []\n",
        "\n",
        "                # Parse label lines\n",
        "                for line in label_lines:\n",
        "                    parts = line.split()\n",
        "                    single_class = int(parts[0])  # First value is the class label\n",
        "                    img_classes.append(single_class)\n",
        "\n",
        "                    single_box = [float(x) * img_size for x in parts[1:]]  # Rest are the boxes (x,y and height,width)\n",
        "                    img_boxes.append(single_box)\n",
        "\n",
        "                classes.append(img_classes)\n",
        "                boxes.append(img_boxes)\n",
        "    return classes, boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x-3weTPFwNkJ"
      },
      "outputs": [],
      "source": [
        "train_classes, train_boxes = get_labels(TRAIN_DIR_LABELS)\n",
        "valid_classes, valid_boxes = get_labels(VALID_DIR_LABELS)\n",
        "test_classes, test_boxes = get_labels(TEST_DIR_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hAu7nJ3iwNkK"
      },
      "outputs": [],
      "source": [
        "def get_imgs(imgs_dir: str) -> (list):\n",
        "    paths = []\n",
        "    for root, dirs, files in os.walk(imgs_dir):\n",
        "        for file in files:\n",
        "            img_path = os.path.join(imgs_dir, os.path.splitext(file)[0] + \".jpg\")\n",
        "            paths.append(img_path)\n",
        "    paths.sort()\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sz3pVclcwNkK"
      },
      "outputs": [],
      "source": [
        "train_img_paths = get_imgs(TRAIN_DIR_IMGS)\n",
        "valid_img_paths = get_imgs(VALID_DIR_IMGS)\n",
        "test_img_paths = get_imgs(TEST_DIR_IMGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYyJsc-DHT4I",
        "outputId": "d97f7f02-e5da-4fb7-ffd8-7da9e6f40e9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14000, 4000)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_img_paths), len(valid_img_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g3MN245TwNkK"
      },
      "outputs": [],
      "source": [
        "# Check correct size\n",
        "assert len(train_classes) == len(train_boxes) == len(train_img_paths)\n",
        "assert len(valid_classes) == len(valid_boxes) == len(valid_img_paths)\n",
        "assert len(test_classes) == len(test_boxes) == len(test_img_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W6aIaHv0wNkK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-04 16:59:17.923482: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
            "2023-09-04 16:59:17.923501: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2023-09-04 16:59:17.923504: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "2023-09-04 16:59:17.923923: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-09-04 16:59:17.924234: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "# Creating data, after converting lists to ragged tensors\n",
        "# Train dataset:\n",
        "train_classes_tf = tf.ragged.constant(train_classes)\n",
        "train_boxes_tf = tf.ragged.constant(train_boxes)\n",
        "train_paths_tf = tf.ragged.constant(train_img_paths)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_paths_tf, train_classes_tf, train_boxes_tf))\n",
        "\n",
        "# Validation dataset:\n",
        "valid_classes_tf = tf.ragged.constant(valid_classes)\n",
        "valid_boxes_tf = tf.ragged.constant(valid_boxes)\n",
        "valid_paths_tf = tf.ragged.constant(valid_img_paths)\n",
        "\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_paths_tf, valid_classes_tf, valid_boxes_tf))\n",
        "\n",
        "# Test dataset:\n",
        "test_classes_tf = tf.ragged.constant(test_classes)\n",
        "test_boxes_tf = tf.ragged.constant(test_boxes)\n",
        "test_paths_tf = tf.ragged.constant(test_img_paths)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_paths_tf, test_classes_tf, test_boxes_tf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y4L2zC77wNkL"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image # Loading data functions\n",
        "\n",
        "\n",
        "def load_dataset(image_path, classes, bbox):\n",
        "    # Read Image\n",
        "    image = load_image(image_path)\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox,\n",
        "    }\n",
        "    return {\"images\": image, \"bounding_boxes\": bounding_boxes}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ucxDrV6mwNkL"
      },
      "outputs": [],
      "source": [
        "# Data augmentater for the training dataset\n",
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"center_xywh\"),\n",
        "        keras_cv.layers.RandomShear(\n",
        "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"center_xywh\"\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(416, 416), scale_factor=(0.75, 1.3), bounding_box_format=\"center_xywh\"\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thMtNByowNkL",
        "outputId": "06d5178f-488a-482f-e91b-6d243173dcac"
      },
      "outputs": [],
      "source": [
        "# Creating training dataset\n",
        "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
        "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i-iIXz96wNkL"
      },
      "outputs": [],
      "source": [
        "# Data resizer for the validation dataset\n",
        "resizing = keras_cv.layers.JitteredResize(\n",
        "    target_size=(416, 416),\n",
        "    scale_factor=(0.75, 1.3),\n",
        "    bounding_box_format=\"center_xywh\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vem8lxOpwNkM"
      },
      "outputs": [],
      "source": [
        "# Creating validation dataset\n",
        "val_ds = valid_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
        "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
        "    inputs = next(iter(inputs.take(1)))\n",
        "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=value_range,\n",
        "        rows=rows,\n",
        "        cols=cols,\n",
        "        y_true=bounding_boxes,\n",
        "        scale=5,\n",
        "        font_scale=0.7,\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        class_mapping=class_mapping,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize_dataset(train_ds, bounding_box_format=\"center_xywh\", value_range=(0,255), rows=2, cols=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_Mq7OcPwNkN"
      },
      "source": [
        "# MODEL CREATION (YOLOv8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMGEptxvwNkO",
        "outputId": "0aa115a0-9994-47eb-faa1-54fb5f87ca9c"
      },
      "outputs": [],
      "source": [
        "# YOLOv8 backbone\n",
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xs_backbone_coco\")  # yolov8 backbone with coco weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NdImlKUSwNkO"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"center_xywh\",\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1, # Feature Pyramid Network\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eC6Gh7o2wNkP"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE, global_clipnorm=GLOBAL_CLIPNORM,)\n",
        "\n",
        "yolo.compile(optimizer=optimizer, classification_loss = \"binary_crossentropy\" , box_loss=\"ciou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pW_QsvSywNkP"
      },
      "outputs": [],
      "source": [
        "# Model evaluation with COCO Metric Callback\n",
        "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, data, save_path):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
        "            bounding_box_format=\"center_xywh\",\n",
        "            evaluate_freq=1e9,\n",
        "        )\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.best_map = -1.0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        self.metrics.reset_state()\n",
        "        for batch in self.data:\n",
        "            images = batch[\"images\"]\n",
        "            bounding_boxes = batch[\"bounding_boxes\"]\n",
        "\n",
        "            # Extract \"boxes\" and \"classes\" from bounding_boxes\n",
        "            classes = bounding_boxes[\"classes\"]\n",
        "            boxes = bounding_boxes[\"boxes\"]\n",
        "\n",
        "            y_pred = self.model.predict(images, verbose=0)\n",
        "\n",
        "            # Convert classes and bounding_boxes to a dictionary\n",
        "            y_true = {\"classes\": classes, \"boxes\": boxes}\n",
        "\n",
        "            self.metrics.update_state(y_true, y_pred)\n",
        "\n",
        "        metrics = self.metrics.result(force=True)\n",
        "        logs.update(metrics)\n",
        "\n",
        "        current_map = metrics[\"MaP\"]\n",
        "        if current_map > self.best_map:\n",
        "            self.best_map = current_map\n",
        "            self.model.save_weights(self.save_path)  # Save the model when mAP improves\n",
        "\n",
        "        return logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQwisPdJwNkP",
        "outputId": "08e9eab0-4903-4740-c2f5-3f6b8b22d726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-04 16:59:24.494451: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-09-04 16:59:24.807249: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-09-04 16:59:30.657289: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        }
      ],
      "source": [
        "# Model training\n",
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=1,\n",
        "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, os.path.join(MODEL_PATH, \"best_coco_model_autosave.keras\")), \n",
        "               tqdm.keras.TQDMCallback(verbose=1)],\n",
        "    verbose=1,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9HBEVKGwNkP",
        "outputId": "f1f03ea8-4408-4697-d836-065648e2d307"
      },
      "outputs": [],
      "source": [
        "# Save the fitted model under name\n",
        "\n",
        "model_save_name = \"final_epoch_model.keras\"\n",
        "\n",
        "yolo.save_weights(os.path.join(MODEL_PATH, model_save_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL3hB2dcR0Os"
      },
      "outputs": [],
      "source": [
        "# Loading the saved model...\n",
        "\n",
        "# Define model architecture\n",
        "def create_custom_model():\n",
        "    backbone = keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xs_backbone_coco\")\n",
        "    yolo = keras_cv.models.YOLOV8Detector(\n",
        "        num_classes=len(class_mapping),\n",
        "        bounding_box_format=\"center_xywh\",\n",
        "        backbone=backbone,\n",
        "        fpn_depth=1,\n",
        "    )\n",
        "    return yolo\n",
        "\n",
        "# Create the model\n",
        "reconstructed_yolo = create_custom_model()\n",
        "\n",
        "# Load the model weights\n",
        "reconstructed_yolo.load_weights(os.path.join(MODEL_PATH, \"best_coco_model_autosave.keras\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGZE6xNcWCuZ",
        "outputId": "4fda9259-f85b-4d79-d6aa-f01f3c7fe13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess your input image\n",
        "image_path = \"/Users/annoskriver/.data/blackjack/original_images/train/images/000090528_jpg.rf.d50e89610e5c97c61632c290692f3e75.jpg\"\n",
        "input_image = load_image(image_path)\n",
        "input_image = tf.expand_dims(input_image, axis=0)  # Add a batch dimension\n",
        "\n",
        "# Make predictions\n",
        "predictions = reconstructed_yolo.predict(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions[\"classes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions[\"images\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYq9U4I9wNkQ"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
